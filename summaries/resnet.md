#Deep Residual Learning for Image Recognition
The authors of this [paper](https://arxiv.org/abs/1512.03385) begin to observe a strange situation. By very deep networks they obtain results poorer than with networks a little shallower. For example they compare a network of 56 layers and a network of 20 and see that the 56 works much worse than 20 layers network. During the work show that as deeper the better this is achieved through the deep residual learning architecture. This architecture contains residual building blocks (ResBlocks). RedBloks are simple layer connected with rectified linear units (ReLU) and a pass-through below that feeds through the information from previous layers unchanged. They also test different modifications of these blocks as bottleneck blocks, with three layers where the middle one restricts the flow of information using fewer inputs and outputs. Another modification is to test different types of pitch connections including a full projection matrix.
The rest of the work tests the performance of the network. And they demonstrate that they can train deeper nets by improving performance. Finally, created a ResNet of 152 layers.
